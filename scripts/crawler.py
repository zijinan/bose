# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½åŒæ³¢æ•°æ®é‡‡é›†çˆ¬è™« - ç²¾åº¦ä¼˜åŒ–ç‰ˆ
åŠŸèƒ½ï¼šé‡‡é›†æŒ‡å®šæœŸæ•°çš„åŒæ³¢å…³é”®è¯å’Œæ³¢è‰²æ•°æ®
ç‰¹æ€§ï¼šä¿æŒé«˜ç²¾åº¦ã€æ€§èƒ½ä¼˜åŒ–ã€é€»è¾‘æ¸…æ™°
"""

import requests
import pandas as pd
import time
import os
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import threading
import chardet
from tqdm import tqdm
from bs4 import BeautifulSoup
from functools import lru_cache


class OptimizedWaveScraper:
    """ç²¾åº¦ä¼˜åŒ–çš„åŒæ³¢æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, target_period, input_file, output_file, batch_size=50, max_workers=8):
        self.input_file = input_file
        self.output_file = output_file
        self.target_period = target_period
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # æ•°æ®å­˜å‚¨
        self.results = []
        self.stats = {'processed': 0, 'success': 0, 'failed': 0}
        self.results_lock = threading.Lock()
        
        # ä¼šè¯æ± 
        self.session_local = threading.local()
        
        # é¢„ç¼–è¯‘æ­£åˆ™æ¨¡å¼
        self._init_patterns()
    
    def _init_patterns(self):
        """åˆå§‹åŒ–æ­£åˆ™æ¨¡å¼"""
        # æœŸæ•°åŒ¹é… - æ›´ç²¾ç¡®çš„æ¨¡å¼
        self.period_pattern = re.compile(r'(\d{1,3})æœŸ')
        
        # æ‰©å±•å…³é”®è¯åˆ—è¡¨ä»¥ä¿æŒç²¾åº¦
        keywords = [
            'ä¸¤æ³¢ä¸­ç‰¹', 'åŒæ³¢ä¸­ç‰¹', 'ç²¾å‡†åŒæ³¢', 'å¿…ä¸­åŒæ³¢', 'åŒæ³¢', 'äºŒæ³¢',
            'ä¸¤æ³¢å¿…ä¸­', 'åŒæ³¢æ¨è', 'ä¸¤æ³¢æ¨è', 'åŒæ³¢ç²¾é€‰', 'ä¸¤æ³¢æ–™',
            'åŒæ³¢æ–™', 'ä¸¤æ³¢çˆ†æ–™', 'åŒæ³¢çˆ†æ–™', 'ä¸¤æ³¢å†…å¹•', 'åŒæ³¢å†…å¹•'
        ]
        keyword_regex = '|'.join(keywords)
        
        # å…³é”®è¯æå– - å¤šç§æ¨¡å¼ç¡®ä¿ç²¾åº¦
        self.keyword_patterns = [
            re.compile(rf'([^\n]*?(?:{keyword_regex})[^\n]*?)', re.IGNORECASE),
            re.compile(rf'<[^>]*>([^<]*?(?:{keyword_regex})[^<]*?)<', re.IGNORECASE),
            re.compile(rf'(?:^|\s)([^\s]*?(?:{keyword_regex})[^\s]*?)(?:\s|$)', re.IGNORECASE)
        ]
        
        # æ³¢è‰²æå– - æ›´å…¨é¢çš„æ¨¡å¼
        self.color_patterns = [
            re.compile(r'([çº¢ç»¿è“]æ³¢)', re.IGNORECASE),
            re.compile(r'(çº¢|ç»¿|è“)(?=æ³¢)', re.IGNORECASE),
            re.compile(r'(?:æ¨è|ç²¾é€‰|å¿…ä¸­).*?([çº¢ç»¿è“]æ³¢)', re.IGNORECASE)
        ]
        
        # å¼€å¥–ç»“æœ - å¤šç§è¡¨è¾¾æ–¹å¼
        self.result_patterns = [
            re.compile(r'å¼€[ï¼š:]?([^\n]*?[ä¸­é”™å‡†])', re.IGNORECASE),
            re.compile(r'ç»“æœ[ï¼š:]?([^\n]*?[ä¸­é”™å‡†])', re.IGNORECASE),
            re.compile(r'([^\n]*?[ä¸­é”™å‡†][^\n]*)', re.IGNORECASE)
        ]
        
        # å…¶ä»–æœŸæ•°æ£€æµ‹ - é˜²æ­¢ä¸²æœŸ
        self.other_period_pattern = re.compile(r'(\d{1,3})æœŸ')
    
    def get_session(self):
        """è·å–çº¿ç¨‹æœ¬åœ°session - ä¼˜åŒ–ç½‘ç»œæ€§èƒ½"""
        if not hasattr(self.session_local, 'session'):
            session = requests.Session()
            # ä¼˜åŒ–sessioné…ç½®
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            })
            # è¿æ¥æ± ä¼˜åŒ–
            adapter = requests.adapters.HTTPAdapter(
                pool_connections=20,
                pool_maxsize=20,
                max_retries=2
            )
            session.mount('http://', adapter)
            session.mount('https://', adapter)
            self.session_local.session = session
        return self.session_local.session
    
    @lru_cache(maxsize=1000)
    def detect_encoding(self, raw_data):
        """æ£€æµ‹ç¼–ç  - ç¼“å­˜ä¼˜åŒ–"""
        try:
            result = chardet.detect(raw_data[:10000])  # åªæ£€æµ‹å‰10KB
            return result.get('encoding', 'utf-8') or 'utf-8'
        except:
            return 'utf-8'
    
    def fetch_url_content(self, url, timeout=15):
        """è·å–URLå†…å®¹ - ä¼˜åŒ–ç‰ˆ"""
        session = self.get_session()
        
        try:
            response = session.get(url, timeout=timeout, stream=True)
            response.raise_for_status()
            
            # è·å–åŸå§‹å†…å®¹
            raw_content = response.content
            
            # æ™ºèƒ½ç¼–ç æ£€æµ‹
            encoding = self.detect_encoding(raw_content)
            
            try:
                content = raw_content.decode(encoding)
            except UnicodeDecodeError:
                # å¤‡ç”¨ç¼–ç 
                for backup_encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                    try:
                        content = raw_content.decode(backup_encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    content = raw_content.decode('utf-8', errors='ignore')
            
            return content
            
        except Exception as e:
            print(f"è·å–URLå¤±è´¥ {url}: {str(e)}")
            return None
    
    def extract_data_with_beautifulsoup(self, html_text):
        """ä½¿ç”¨BeautifulSoupæå–æ•°æ® - é«˜ç²¾åº¦æ–¹æ³•"""
        try:
            soup = BeautifulSoup(html_text, 'html.parser')
            contexts = []
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            text_content = soup.get_text()
            
            # æŒ‰è¡Œåˆ†å‰²å¹¶è¿‡æ»¤
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
            
            # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æœŸæ•°çš„è¡ŒåŠå…¶ä¸Šä¸‹æ–‡
            target_lines = []
            for i, line in enumerate(lines):
                if f"{self.target_period}æœŸ" in line:
                    # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„3è¡Œï¼‰
                    start_idx = max(0, i - 3)
                    end_idx = min(len(lines), i + 4)
                    context = '\n'.join(lines[start_idx:end_idx])
                    target_lines.append(context)
            
            return target_lines
            
        except Exception as e:
            print(f"BeautifulSoupè§£æå¤±è´¥: {str(e)}")
            return []
    
    def extract_period_context(self, text, target_period):
        """æå–æŒ‡å®šæœŸæ•°çš„ä¸Šä¸‹æ–‡ - å¤‡ç”¨æ–¹æ³•"""
        contexts = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            if f"{target_period}æœŸ" in line:
                # è·å–ä¸Šä¸‹æ–‡
                start_idx = max(0, i - 5)
                end_idx = min(len(lines), i + 6)
                context = '\n'.join(lines[start_idx:end_idx])
                contexts.append(context)
        
        return contexts
    
    def extract_keywords(self, context):
        """æå–å…³é”®è¯ - å¤šæ¨¡å¼åŒ¹é…"""
        keywords = set()
        
        for pattern in self.keyword_patterns:
            matches = pattern.findall(context)
            for match in matches:
                clean_keyword = re.sub(r'<[^>]*>', '', match).strip()
                if clean_keyword and len(clean_keyword) > 2:
                    keywords.add(clean_keyword)
        
        return list(keywords)
    
    def extract_colors(self, context):
        """æå–æ³¢è‰² - å¢å¼ºç‰ˆå¤šæ¨¡å¼åŒ¹é…"""
        colors = set()
        
        # åŸæœ‰çš„æ³¢è‰²æ¨¡å¼åŒ¹é…
        for pattern in self.color_patterns:
            matches = pattern.findall(context)
            for match in matches:
                if match.endswith('æ³¢'):
                    colors.add(match)
                else:
                    colors.add(match + 'æ³¢')
        
        # æ–°å¢ï¼šå•å­—ç¬¦æ³¢è‰²è¯†åˆ«ï¼ˆå¤„ç†çº¢ç»¿ã€â™ ç»¿çº¢â™ ã€ã€è“çº¢æ³¢ã€‘ã€[ è“çº¢ ]ç­‰æ ¼å¼ï¼‰
        # ç§»é™¤ç‰¹æ®Šç¬¦å·åæŸ¥æ‰¾æ³¢è‰²å­—ç¬¦
        cleaned_text = re.sub(r'[â™ ã€ã€‘\[\]()ï¼ˆï¼‰<>ã€Šã€‹ğŸ¬æ³¢]', '', context)
        
        # æŸ¥æ‰¾æ‰€æœ‰æ³¢è‰²å­—ç¬¦
        color_chars = re.findall(r'[çº¢ç»¿è“]', cleaned_text)
        
        # è½¬æ¢ä¸ºå®Œæ•´æ³¢è‰²åç§°å¹¶å»é‡
        color_map = {'çº¢': 'çº¢æ³¢', 'ç»¿': 'ç»¿æ³¢', 'è“': 'è“æ³¢'}
        seen_chars = set()
        
        for char in color_chars:
            if char in color_map and char not in seen_chars:
                colors.add(color_map[char])
                seen_chars.add(char)
        
        # é™åˆ¶æœ€å¤šè¿”å›3ä¸ªæ³¢è‰²
        return list(colors)[:3]
    
    def extract_results(self, context):
        """æå–å¼€å¥–ç»“æœ"""
        results = set()
        
        for pattern in self.result_patterns:
            matches = pattern.findall(context)
            for match in matches:
                clean_result = re.sub(r'<[^>]*>', '', match).strip()
                if clean_result and len(clean_result) <= 10:
                    results.add(clean_result)
        
        return list(results)
    
    def parse_single_period(self, html_text):
        """è§£æå•ä¸ªæœŸæ•°çš„æ•°æ® - æ··åˆæ–¹æ³•"""
        all_data = []
        
        # æ–¹æ³•1ï¼šBeautifulSoupè§£æï¼ˆé«˜ç²¾åº¦ï¼‰
        bs_contexts = self.extract_data_with_beautifulsoup(html_text)
        
        # æ–¹æ³•2ï¼šæ­£åˆ™è¡¨è¾¾å¼è§£æï¼ˆå¤‡ç”¨ï¼‰
        regex_contexts = self.extract_period_context(html_text, self.target_period)
        
        # åˆå¹¶ä¸Šä¸‹æ–‡
        contexts = bs_contexts if bs_contexts else (regex_contexts or [])
        
        for context in contexts:
            keywords = self.extract_keywords(context)
            colors = self.extract_colors(context)
            results = self.extract_results(context)
            
            # è´¨é‡è¿‡æ»¤ - é¿å…é‡‡é›†å¹¿å‘Šå’Œæ— å…³å†…å®¹
            if self.is_valid_content(context, keywords, colors):
                data = {
                    'æœŸæ•°': f"{self.target_period}æœŸ",
                    'å…³é”®è¯': ', '.join(keywords),
                    'æ³¢è‰²': ', '.join(colors) if colors else '',
                    'å¼€å¥–ç»“æœ': ', '.join(results) if results else ''
                }
                all_data.append(data)
        
        return all_data
    
    def is_valid_content(self, context, keywords, colors):
        """åˆ¤æ–­å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼Œè¿‡æ»¤å¹¿å‘Šå’Œæ— å…³å†…å®¹"""
        # å¿…é¡»æœ‰å…³é”®è¯æ‰ç®—æœ‰æ•ˆ
        if not keywords:
            return False
        
        # è¿‡æ»¤æ˜æ˜¾çš„å¹¿å‘Šå†…å®¹
        ad_keywords = [
            'å¹¿å‘Š', 'æ¨å¹¿', 'è”ç³»', 'å¾®ä¿¡', 'QQ', 'ç”µè¯', 'æ‰‹æœº', 'ç½‘å€', 'http',
            'åŠ ç¾¤', 'å®¢æœ', 'å’¨è¯¢', 'ä»£ç†', 'æ‹›å•†', 'åˆä½œ', 'æŠ•èµ„', 'ç†è´¢',
            'è´·æ¬¾', 'å€Ÿé’±', 'èµšé’±', 'å…¼èŒ', 'æ‹›è˜', 'å·¥ä½œ', 'èŒä½',
            'å…è´¹', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'ç‰¹ä»·', 'ä¿ƒé”€', 'æ´»åŠ¨', 'ç¤¼å“',
            'æ³¨å†Œ', 'å¼€æˆ·', 'å……å€¼', 'æç°', 'è½¬è´¦', 'æ”¯ä»˜', 'é“¶è¡Œ',
            'ç‚¹å‡»', 'ä¸‹è½½', 'å®‰è£…', 'æ‰«ç ', 'å…³æ³¨', 'è®¢é˜…'
        ]
        
        context_lower = context.lower()
        for ad_word in ad_keywords:
            if ad_word in context_lower:
                return False
        
        # è¿‡æ»¤çº¯æ ‡é¢˜å†…å®¹ï¼ˆé€šå¸¸å¾ˆçŸ­ä¸”æ²¡æœ‰å®è´¨å†…å®¹ï¼‰
        if len(context.strip()) < 20:
            return False
        
        # è¿‡æ»¤é‡å¤å­—ç¬¦è¿‡å¤šçš„å†…å®¹
        if self.has_excessive_repetition(context):
            return False
        
        return True
    
    def has_excessive_repetition(self, text):
        """æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šé‡å¤å­—ç¬¦"""
        if len(text) < 10:
            return False
        
        # æ£€æŸ¥è¿ç»­é‡å¤å­—ç¬¦
        for i in range(len(text) - 5):
            char = text[i]
            if char != ' ' and text[i:i+6] == char * 6:
                return True
        
        return False
    
    def parse_page_data(self, html_text, url):
        """è§£æé¡µé¢æ•°æ®"""
        try:
            data = self.parse_single_period(html_text)
            if data:
                print(f"âœ“ {url} - æ‰¾åˆ° {len(data)} æ¡æœ‰æ•ˆæ•°æ®")
            return data
        except Exception as e:
            print(f"âš ï¸ è§£æé”™è¯¯ {url}: {str(e)[:50]}...")
            return []
    
    def process_single_url(self, url):
        """å¤„ç†å•ä¸ªURL"""
        try:
            html_text = self.fetch_url_content(url)
            if html_text:
                data = self.parse_page_data(html_text, url)
                return True, data
            return False, []
        except Exception:
            return False, []
    
    def save_results(self, incremental=False):
        """ä¿å­˜ç»“æœ"""
        if self.results:
            try:
                df = pd.DataFrame(self.results)
                if incremental:
                    # å¢é‡ä¿å­˜ï¼šè¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
                    if os.path.exists(self.output_file):
                        df.to_csv(self.output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
                    else:
                        df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
                    print(f"ğŸ’¾ å¢é‡ä¿å­˜ {len(self.results)} æ¡æ•°æ®åˆ°: {self.output_file}")
                    # æ¸…ç©ºå·²ä¿å­˜çš„æ•°æ®
                    self.results.clear()
                else:
                    # å®Œæ•´ä¿å­˜
                    df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
                    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.output_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def process_urls_batch(self, urls):
        """æ‰¹é‡å¤„ç†URLs"""
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_url = {executor.submit(self.process_single_url, url): url for url in urls}
            
            for future in as_completed(future_to_url, timeout=300):
                url = future_to_url[future]
                try:
                    success, data = future.result(timeout=30)
                    
                    with self.results_lock:
                        self.stats['processed'] += 1
                        if success:
                            self.stats['success'] += 1
                            if data:
                                self.results.extend(data)
                        else:
                            self.stats['failed'] += 1
                            
                except TimeoutError:
                    print(f"â° è¶…æ—¶: {url}")
                    with self.results_lock:
                        self.stats['failed'] += 1
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥ {url}: {str(e)[:50]}...")
                    with self.results_lock:
                        self.stats['failed'] += 1
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        try:
            # è¯»å–é“¾æ¥
            df = pd.read_csv(self.input_file)
            urls = df['link'].tolist()
            total_urls = len(urls)
            
            print(f"ğŸ“Š å¼€å§‹é‡‡é›† {self.target_period}æœŸ æ•°æ®")
            print(f"ğŸ“‹ æ€»é“¾æ¥æ•°: {total_urls}")
            print(f"ğŸ”§ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"ğŸ§µ å¹¶å‘æ•°: {self.max_workers}")
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, total_urls, self.batch_size):
                batch_urls = urls[i:i + self.batch_size]
                batch_num = i // self.batch_size + 1
                total_batches = (total_urls + self.batch_size - 1) // self.batch_size
                
                print(f"\nğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_urls)} ä¸ªé“¾æ¥)")
                
                self.process_urls_batch(batch_urls)
                
                # å¢é‡ä¿å­˜
                if self.results:
                    self.save_results(incremental=True)
                
                # æ˜¾ç¤ºè¿›åº¦
                print(f"ğŸ“ˆ è¿›åº¦: {self.stats['processed']}/{total_urls} | "
                      f"æˆåŠŸ: {self.stats['success']} | å¤±è´¥: {self.stats['failed']}")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + self.batch_size < total_urls:
                    time.sleep(2)
            
            # æœ€ç»ˆä¿å­˜
            self.save_results()
            
            print(f"\nğŸ‰ é‡‡é›†å®Œæˆ!")
            print(f"ğŸ“Š æ€»å¤„ç†: {self.stats['processed']}")
            print(f"âœ… æˆåŠŸ: {self.stats['success']}")
            print(f"âŒ å¤±è´¥: {self.stats['failed']}")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {self.output_file}")
            
        except Exception as e:
            print(f"âŒ è¿è¡Œå¤±è´¥: {e}")
            # ä¿å­˜å·²é‡‡é›†çš„æ•°æ®
            if self.results:
                self.save_results()


if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python crawler.py <æœŸæ•°>")
        print("ç¤ºä¾‹: python crawler.py 160")
        sys.exit(1)
    
    try:
        target_period = int(sys.argv[1])
    except ValueError:
        print("âŒ æœŸæ•°å¿…é¡»æ˜¯æ•°å­—")
        sys.exit(1)
    
    # æ–‡ä»¶è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    input_file = os.path.join(current_dir, '..', 'data', 'links_processed.csv')
    
    # å¦‚æœé¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶
    if not os.path.exists(input_file):
        input_file = os.path.join(current_dir, '..', 'data', 'links.csv')
    
    output_file = os.path.join(current_dir, '..', 'data', f'results_{target_period}.csv')
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹å¹¶è¿è¡Œ
    scraper = OptimizedWaveScraper(
        target_period=target_period,
        input_file=input_file,
        output_file=output_file,
        batch_size=50,
        max_workers=8
    )
    
    scraper.run()