#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜æ€§èƒ½åŒæ³¢æ•°æ®é‡‡é›†çˆ¬è™« - ç²¾åº¦ä¼˜åŒ–ç‰ˆ
åŠŸèƒ½ï¼šé‡‡é›†æŒ‡å®šæœŸæ•°çš„åŒæ³¢å…³é”®è¯å’Œæ³¢è‰²æ•°æ®
ç‰¹æ€§ï¼šä¿æŒé«˜ç²¾åº¦ã€æ€§èƒ½ä¼˜åŒ–ã€é€»è¾‘æ¸…æ™°
"""

import requests
import pandas as pd
import time
import os
import re
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
import threading
import chardet
from tqdm import tqdm
from bs4 import BeautifulSoup
from functools import lru_cache


class OptimizedWaveScraper:
    """ç²¾åº¦ä¼˜åŒ–çš„åŒæ³¢æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, target_period, input_file, output_file, batch_size=50, max_workers=8):
        self.input_file = input_file
        self.output_file = output_file
        self.target_period = target_period
        self.batch_size = batch_size
        self.max_workers = max_workers
        
        # æ•°æ®å­˜å‚¨
        self.results = []
        self.stats = {'processed': 0, 'success': 0, 'failed': 0}
        self.results_lock = threading.Lock()
        
        # ä¼šè¯æ± 
        self.session_local = threading.local()
        
        # é¢„ç¼–è¯‘æ­£åˆ™æ¨¡å¼
        self._init_patterns()
    
    def _init_patterns(self):
        """åˆå§‹åŒ–æ­£åˆ™æ¨¡å¼"""
        # æœŸæ•°åŒ¹é… - æ›´ç²¾ç¡®çš„æ¨¡å¼
        self.period_pattern = re.compile(r'(\d{1,3})æœŸ')
        
        # æ‰©å±•å…³é”®è¯åˆ—è¡¨ä»¥ä¿æŒç²¾åº¦
        keywords = [
            'ä¸¤æ³¢ä¸­ç‰¹', 'åŒæ³¢ä¸­ç‰¹', 'ç²¾å‡†åŒæ³¢', 'å¿…ä¸­åŒæ³¢', 'åŒæ³¢', 'äºŒæ³¢',
            'ä¸¤æ³¢å¿…ä¸­', 'åŒæ³¢æ¨è', 'ä¸¤æ³¢æ¨è', 'åŒæ³¢ç²¾é€‰', 'ä¸¤æ³¢æ–™',
            'åŒæ³¢æ–™', 'ä¸¤æ³¢çˆ†æ–™', 'åŒæ³¢çˆ†æ–™', 'ä¸¤æ³¢å†…å¹•', 'åŒæ³¢å†…å¹•'
        ]
        keyword_regex = '|'.join(keywords)
        
        # å…³é”®è¯æå– - å¤šç§æ¨¡å¼ç¡®ä¿ç²¾åº¦
        self.keyword_patterns = [
            re.compile(rf'([^\n]*?(?:{keyword_regex})[^\n]*?)', re.IGNORECASE),
            re.compile(rf'<[^>]*>([^<]*?(?:{keyword_regex})[^<]*?)<', re.IGNORECASE),
            re.compile(rf'(?:^|\s)([^\s]*?(?:{keyword_regex})[^\s]*?)(?:\s|$)', re.IGNORECASE)
        ]
        
        # æ³¢è‰²æå– - æ›´å…¨é¢çš„æ¨¡å¼
        self.color_patterns = [
            re.compile(r'([çº¢ç»¿è“]æ³¢)', re.IGNORECASE),
            re.compile(r'(çº¢|ç»¿|è“)(?=æ³¢)', re.IGNORECASE),
            re.compile(r'(?:æ¨è|ç²¾é€‰|å¿…ä¸­).*?([çº¢ç»¿è“]æ³¢)', re.IGNORECASE)
        ]
        
        # å¼€å¥–ç»“æœ - å¤šç§è¡¨è¾¾æ–¹å¼
        self.result_patterns = [
            re.compile(r'å¼€[ï¼š:]?([^\n]*?[ä¸­é”™å‡†])', re.IGNORECASE),
            re.compile(r'ç»“æœ[ï¼š:]?([^\n]*?[ä¸­é”™å‡†])', re.IGNORECASE),
            re.compile(r'([^\n]*?[ä¸­é”™å‡†][^\n]*)', re.IGNORECASE)
        ]
        
        # å…¶ä»–æœŸæ•°æ£€æµ‹ - é˜²æ­¢ä¸²æœŸ
        self.other_period_pattern = re.compile(r'(\d{1,3})æœŸ')
    
    def get_session(self):
        """è·å–çº¿ç¨‹æœ¬åœ°session - ä¼˜åŒ–ç½‘ç»œæ€§èƒ½"""
        if not hasattr(self.session_local, 'session'):
            session = requests.Session()
            # ä¼˜åŒ–sessioné…ç½®
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            })
            # è¿æ¥æ± ä¼˜åŒ–
            adapter = requests.adapters.HTTPAdapter(
                pool_connections=20,
                pool_maxsize=20,
                max_retries=2
            )
            session.mount('http://', adapter)
            session.mount('https://', adapter)
            self.session_local.session = session
        return self.session_local.session
    
    def load_links(self):
        """åŠ è½½é“¾æ¥æ–‡ä»¶"""
        try:
            df = pd.read_csv(self.input_file)
            # æ”¯æŒå¤šç§åˆ—å
            link_columns = ['link', 'url', 'links', 'urls']
            link_column = None
            
            for col in link_columns:
                if col in df.columns:
                    link_column = col
                    break
            
            if link_column is None:
                # ä½¿ç”¨ç¬¬ä¸€åˆ—
                links = df.iloc[:, 0].dropna().tolist()
            else:
                links = df[link_column].dropna().tolist()
            
            valid_links = [link for link in links if isinstance(link, str) and link.startswith(('http://', 'https://'))]
            print(f"åŠ è½½äº† {len(valid_links)} ä¸ªæœ‰æ•ˆé“¾æ¥")
            return valid_links
        except Exception as e:
            print(f"åŠ è½½é“¾æ¥æ–‡ä»¶é”™è¯¯: {e}")
            return []
    
    def get_page_content(self, url):
        """è·å–ç½‘é¡µå†…å®¹ - ä¼˜åŒ–ç‰ˆ"""
        try:
            session = self.get_session()
            response = session.get(url, timeout=(5, 10))  # è¿æ¥è¶…æ—¶5ç§’ï¼Œè¯»å–è¶…æ—¶10ç§’
            response.raise_for_status()
            
            # æ£€æŸ¥å“åº”å¤§å°
            if len(response.content) > 5 * 1024 * 1024:  # 5MBé™åˆ¶
                return None
            
            # ç¼–ç æ£€æµ‹ä¼˜åŒ–
            if response.encoding in ['ISO-8859-1', None]:
                detected = chardet.detect(response.content[:2048])  # å¢åŠ æ£€æµ‹æ ·æœ¬
                if detected.get('confidence', 0) > 0.7:
                    response.encoding = detected['encoding']
                else:
                    response.encoding = 'utf-8'  # é»˜è®¤ç¼–ç 
            
            return response.text
        except requests.exceptions.Timeout:
            return None
        except requests.exceptions.RequestException:
            return None
        except Exception:
            return None
    
    @lru_cache(maxsize=1000)
    def extract_period_context(self, html_text, target_period):
        """æå–æœŸæ•°ä¸Šä¸‹æ–‡ - ç¼“å­˜ä¼˜åŒ–"""
        period_matches = list(self.period_pattern.finditer(html_text))
        target_matches = [match for match in period_matches if int(match.group(1)) == target_period]
        
        if not target_matches:
            return None
        
        contexts = []
        for match in target_matches:
            position = match.start()
            # åŠ¨æ€è°ƒæ•´ä¸Šä¸‹æ–‡èŒƒå›´
            start = max(0, position - 300)
            end = min(len(html_text), position + 300)
            context = html_text[start:end]
            
            # æ£€æŸ¥ä¸²æœŸé£é™©
            other_periods = [int(p) for p in self.other_period_pattern.findall(context) if int(p) != target_period]
            if len(other_periods) <= 1:  # å…è®¸å°‘é‡å…¶ä»–æœŸæ•°
                contexts.append(context)
        
        return contexts if contexts else None
    
    def extract_data_with_beautifulsoup(self, html_text):
        """ä½¿ç”¨BeautifulSoupæå–æ•°æ® - ä¿æŒç²¾åº¦"""
        try:
            # ä½¿ç”¨lxmlè§£æå™¨æé«˜æ€§èƒ½
            soup = BeautifulSoup(html_text, 'lxml')
            
            # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æœŸæ•°çš„å…ƒç´ 
            period_elements = soup.find_all(text=re.compile(f'{self.target_period}æœŸ'))
            
            results = []
            for element in period_elements:
                # è·å–çˆ¶å…ƒç´ çš„æ–‡æœ¬å†…å®¹
                parent = element.parent
                if parent:
                    # å°è¯•è·å–è¡¨æ ¼è¡Œæˆ–æ®µè½
                    row = parent.find_parent(['tr', 'p', 'div', 'td'])
                    if row:
                        context = row.get_text(separator=' ', strip=True)
                        
                        # æ£€æŸ¥ä¸²æœŸ
                        other_periods = [int(p) for p in self.other_period_pattern.findall(context) if int(p) != self.target_period]
                        if len(other_periods) <= 1:
                            results.append(context)
            
            return results
        except Exception:
            return []
    
    def extract_keywords(self, context):
        """æå–å…³é”®è¯ - å¤šæ¨¡å¼åŒ¹é…"""
        keywords = set()
        
        for pattern in self.keyword_patterns:
            matches = pattern.findall(context)
            for match in matches:
                clean_match = re.sub(r'<[^>]*>', '', match).strip()
                clean_match = re.sub(r'[\s\u3000]+', ' ', clean_match)  # æ¸…ç†ç©ºç™½å­—ç¬¦
                if clean_match and len(clean_match) > 2:
                    keywords.add(clean_match)
        
        return list(keywords)
    
    def extract_colors(self, context):
        """æå–æ³¢è‰² - å¢å¼ºç‰ˆå¤šæ¨¡å¼åŒ¹é…"""
        colors = set()
        
        # åŸæœ‰çš„æ³¢è‰²æ¨¡å¼åŒ¹é…
        for pattern in self.color_patterns:
            matches = pattern.findall(context)
            for match in matches:
                if match.endswith('æ³¢'):
                    colors.add(match)
                else:
                    colors.add(match + 'æ³¢')
        
        # æ–°å¢ï¼šå•å­—ç¬¦æ³¢è‰²è¯†åˆ«ï¼ˆå¤„ç†çº¢ç»¿ã€â™ ç»¿çº¢â™ ç­‰æ ¼å¼ï¼‰
        # ç§»é™¤ç‰¹æ®Šç¬¦å·åæŸ¥æ‰¾æ³¢è‰²å­—ç¬¦
        cleaned_text = re.sub(r'[â™ ã€ã€‘\[\]()ï¼ˆï¼‰<>ã€Šã€‹""''
    
    def extract_results(self, context):
        """æå–å¼€å¥–ç»“æœ - å¤šæ¨¡å¼åŒ¹é…"""
        results = set()
        
        for pattern in self.result_patterns:
            matches = pattern.findall(context)
            for match in matches:
                clean_result = re.sub(r'<[^>]*>', '', match).strip()
                if clean_result and len(clean_result) > 1:
                    results.add(clean_result)
        
        return list(results)
    
    def parse_single_period(self, html_text):
        """è§£æå•ä¸ªæœŸæ•°çš„æ•°æ® - æ··åˆæ–¹æ³•"""
        all_data = []
        
        # æ–¹æ³•1ï¼šBeautifulSoupè§£æï¼ˆé«˜ç²¾åº¦ï¼‰
        bs_contexts = self.extract_data_with_beautifulsoup(html_text)
        
        # æ–¹æ³•2ï¼šæ­£åˆ™è¡¨è¾¾å¼è§£æï¼ˆå¤‡ç”¨ï¼‰
        regex_contexts = self.extract_period_context(html_text, self.target_period)
        
        # åˆå¹¶ä¸Šä¸‹æ–‡
        contexts = bs_contexts if bs_contexts else (regex_contexts or [])
        
        for context in contexts:
            keywords = self.extract_keywords(context)
            colors = self.extract_colors(context)
            results = self.extract_results(context)
            
            # è´¨é‡è¿‡æ»¤
            if keywords:  # å¿…é¡»æœ‰å…³é”®è¯
                data = {
                    'æœŸæ•°': f"{self.target_period}æœŸ",
                    'å…³é”®è¯': ', '.join(keywords),
                    'æ³¢è‰²': ', '.join(colors) if colors else '',
                    'å¼€å¥–ç»“æœ': ', '.join(results) if results else ''
                }
                all_data.append(data)
        
        return all_data
    
    def parse_page_data(self, html_text, url):
        """è§£æé¡µé¢æ•°æ®"""
        try:
            data = self.parse_single_period(html_text)
            if data:
                print(f"âœ“ {url} - æ‰¾åˆ° {len(data)} æ¡æ•°æ®")
            return data
        except Exception as e:
            print(f"âš ï¸ è§£æé”™è¯¯ {url}: {str(e)[:50]}...")
            return []
    
    def process_single_url(self, url):
        """å¤„ç†å•ä¸ªURL"""
        try:
            html_text = self.get_page_content(url)
            if html_text:
                data = self.parse_page_data(html_text, url)
                return True, data
            return False, []
        except Exception:
            return False, []
    
    def save_results(self, incremental=False):
        """ä¿å­˜ç»“æœ"""
        if self.results:
            try:
                df = pd.DataFrame(self.results)
                if incremental:
                    # å¢é‡ä¿å­˜ï¼šè¿½åŠ åˆ°ç°æœ‰æ–‡ä»¶
                    if os.path.exists(self.output_file):
                        df.to_csv(self.output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
                    else:
                        df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
                    print(f"ğŸ’¾ å¢é‡ä¿å­˜ {len(self.results)} æ¡æ•°æ®åˆ°: {self.output_file}")
                    # æ¸…ç©ºå·²ä¿å­˜çš„æ•°æ®
                    self.results.clear()
                else:
                    # å®Œæ•´ä¿å­˜
                    df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
                    print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.output_file}")
            except Exception as e:
                print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        else:
            if not incremental:
                print("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print(f"ğŸš€ å¼€å§‹é‡‡é›†ç¬¬ {self.target_period} æœŸæ•°æ®")
        print(f"æ‰¹å¤„ç†å¤§å°: {self.batch_size}, çº¿ç¨‹æ•°: {self.max_workers}")
        
        links = self.load_links()
        if not links:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„é“¾æ¥")
            return
        
        start_time = time.time()
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(links), self.batch_size):
                batch_links = links[i:i + self.batch_size]
                batch_num = i // self.batch_size + 1
                total_batches = (len(links) - 1) // self.batch_size + 1
                
                print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}")
                
                with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                    future_to_url = {executor.submit(self.process_single_url, url): url for url in batch_links}
                    
                    with tqdm(total=len(batch_links), desc="å½“å‰æ‰¹æ¬¡") as pbar:
                        for future in as_completed(future_to_url):
                            self.stats['processed'] += 1
                            
                            try:
                                success, data = future.result(timeout=15)
                                if success and data:
                                    with self.results_lock:
                                        self.results.extend(data)
                                    self.stats['success'] += 1
                                else:
                                    self.stats['failed'] += 1
                            except TimeoutError:
                                self.stats['failed'] += 1
                            except Exception:
                                self.stats['failed'] += 1
                            
                            pbar.update(1)
                            pbar.set_postfix({
                                'æˆåŠŸ': self.stats['success'],
                                'æ•°æ®': len(self.results)
                            })
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¢é‡ä¿å­˜ï¼ˆæ¯50æ¡æ•°æ®ä¿å­˜ä¸€æ¬¡ï¼‰
                if len(self.results) >= 50:
                    with self.results_lock:
                        self.save_results(incremental=True)
        
        except KeyboardInterrupt:
            print("\nâš ï¸ ç¨‹åºè¢«ä¸­æ–­")
        finally:
            # ä¿å­˜å‰©ä½™æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
            if self.results:
                self.save_results()
            
            # æ˜¾ç¤ºç»Ÿè®¡
            elapsed = time.time() - start_time
            success_rate = self.stats['success'] / self.stats['processed'] * 100 if self.stats['processed'] > 0 else 0
            
            print(f"\nğŸ“Š é‡‡é›†å®Œæˆ:")
            print(f"å¤„ç†é“¾æ¥: {self.stats['processed']} | æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"æœ‰æ•ˆæ•°æ®: {len(self.results)} æ¡ | è€—æ—¶: {elapsed:.1f} ç§’")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            # ä»å‘½ä»¤è¡Œå‚æ•°è·å–æœŸæ•°
            try:
                target_period = int(sys.argv[1])
            except ValueError:
                print("âŒ æœŸæ•°æ ¼å¼é”™è¯¯")
                return
        else:
            # äº¤äº’å¼è¾“å…¥æœŸæ•°
            target_period = input("è¯·è¾“å…¥ç›®æ ‡æœŸæ•° (å¦‚: 160): ").strip()
            if not target_period:
                print("âŒ æœŸæ•°ä¸èƒ½ä¸ºç©º")
                return
            
            # è½¬æ¢ä¸ºæ•´æ•°
            try:
                target_period = int(target_period)
            except ValueError:
                print("âŒ æœŸæ•°æ ¼å¼é”™è¯¯")
                return
        
        # ä½¿ç”¨é€šç”¨çš„links.csvæ–‡ä»¶
        input_file = "d:/6zai/bose/data/links.csv"
        output_file = f"d:/6zai/bose/data/results_{target_period}.csv"
        
        scraper = OptimizedWaveScraper(
            target_period=target_period,
            input_file=input_file,
            output_file=output_file,
            batch_size=50,
            max_workers=8
        )
        
        scraper.run()
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    main()